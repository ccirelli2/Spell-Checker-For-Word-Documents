{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe purpose of this code is to classify legal text by type.\\n\\nTypes include:\\n1.) Complaint\\n2.) Order\\n3.) Summary judgement\\n4.) Cover sheet.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The purpose of this code is to classify legal text by type.\n",
    "\n",
    "Types include:\n",
    "1.) Complaint\n",
    "2.) Order\n",
    "3.) Summary judgement\n",
    "4.) Cover sheet.\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TARGET DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Note:    The target directory should be changed by the user to point to the directory within which they have saved their\n",
    "            text files'''\n",
    "\n",
    "os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "Dir_list = os.listdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT CLEANING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Note:    These are the cleaning modules that were created to clean the text of obvious errors or words that will likely \n",
    "            not be material to our ultimate analysis.\n",
    "            These functions constitute the underlying code for the subsequent text-cleaning pipeline program that is used \n",
    "            in the ultimate code (see end of document)'''\n",
    "\n",
    "def clean_text_4_classification_remove_backslashes(Text_file):\n",
    "    '''The purpose of this function is to clean the text files of numerous instances of backslashes \n",
    "    in order to prepare them for the regex expression search. \n",
    "    Input  =   Single text file \n",
    "    Output =   Single text file cleaned \n",
    "    '''\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    Text_file_lower = Text_file.lower()\n",
    "    \n",
    "    # Split any values in the text on the backslash.  The Text_split_slash should return a list. \n",
    "    Text_split_slash = Text_file_lower.split('\\\\')\n",
    "        \n",
    "    # Return the list to a text. \n",
    "    Text_rejoined = ' '.join(Text_split_slash)\n",
    "                \n",
    "    # Return a list of the cleaned text docs. \n",
    "    return Text_rejoined\n",
    "\n",
    "\n",
    "def clean_text_4_classification_remove_nABC(Text_file):\n",
    "    '''The purpose of this function is to remove the 'n' that appears before words that begin with an upper case letter.  \n",
    "    Input  =   Single txt file\n",
    "    Output =   Clean list of tokens from original txt file\n",
    "    '''\n",
    "    # Define the regex expression that you want to search for. \n",
    "    Regex_exp = re.compile('n[A-Z*]')\n",
    "    \n",
    "    # Create a list to capture the tokens once they are cleaned \n",
    "    Text_tokenized_cleaned = []\n",
    "            \n",
    "    # Tokenize the given text\n",
    "    Text_tokenized = nltk.word_tokenize(Text_file)\n",
    "            \n",
    "    # Run for loop over tokens for a given text. \n",
    "    for token in Text_tokenized:\n",
    "\n",
    "        # Search for the regex expression\n",
    "        Regex_search = re.search(Regex_exp, token)\n",
    "                \n",
    "        # Test if there was match (None = no match)\n",
    "        if Regex_search != None:\n",
    "                     \n",
    "            # If there was a match, take all letters after the 'n'.   \n",
    "            token_cleaned = token[1:]\n",
    "                    \n",
    "            Text_tokenized_cleaned.append(token_cleaned)\n",
    "                        \n",
    "        # If the Regex_search returned None, return the token back to the Text_tokenized_cleaned list\n",
    "        else:\n",
    "            Text_tokenized_cleaned.append(token)\n",
    "    \n",
    "    # Return a list of clean tokens\n",
    "    return Text_tokenized_cleaned\n",
    "\n",
    "\n",
    "\n",
    "def create_dict_punct():\n",
    "    '''The purpose of this function is to simply create a dictionary of punctuation symbols to use\n",
    "    in other functions\n",
    "    Input  = None\n",
    "    Output = Dict whose keys are the distinct punctuation marks. \n",
    "    '''\n",
    "    import string\n",
    "    Dict = {}\n",
    "    Punct = string.punctuation\n",
    "    for x in Punct:\n",
    "        Dict[x] = ''\n",
    "    return Dict \n",
    "\n",
    "def strip_punctuation(Token_list):\n",
    "    '''The purpose of this function is to strip the punctuation from a list of tokens. \n",
    "    Input  =  List of tokens\n",
    "    Output =  List of tokens absent punctuation.  \n",
    "    '''\n",
    "    # Import punctuation dictionary\n",
    "    Dict_punct = create_dict_punct()\n",
    "\n",
    "    # Create a list to capture the cleaned tokens\n",
    "    Clean_token_list = []    \n",
    "        \n",
    "    # Iterate over the tokens in the txt file\n",
    "    for x in Token_list:\n",
    "        if x not in Dict_punct:\n",
    "            # Append tokens to clean token list\n",
    "            Clean_token_list.append(x)\n",
    "    \n",
    "    # Return a list of cleaned text\n",
    "    return Clean_token_list\n",
    "\n",
    "def strip_two_letter_words(Token_list):\n",
    "    '''The purpose of this function is to remove any two letter tokens from a list of tokens.\n",
    "    Input  =   List of tokens\n",
    "    Output =   List of tokens absent two letter words'''\n",
    "    \n",
    "    List = [x for x in Token_list if len(x) > 2]\n",
    "    \n",
    "    return List\n",
    "\n",
    "def create_dict_stopwords():\n",
    "    '''The purpose of this code is to create a dictionary of stop words. \n",
    "    Input  = None\n",
    "    Output = Dictionary of stop words'''\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    Stopwords = stopwords.words('english')                  \n",
    "    Dict = {}\n",
    "    for x in Stopwords:\n",
    "        Dict[x] = ''\n",
    "    return Dict\n",
    "\n",
    "def strip_stop_words(Token_list):\n",
    "    ''' The purpose of this code is to strip the stop words from a given text\n",
    "    Input  = List of tokens \n",
    "    Outpu  = Text clean of stop words'''\n",
    "    \n",
    "    stop_words = create_dict_stopwords()\n",
    "    List = []\n",
    "    for x in Token_list:\n",
    "        if x not in stop_words:\n",
    "            List.append(x)\n",
    "    return List\n",
    "\n",
    "def create_Concatenated_text_file(Dir_list, New_file_name):     \n",
    "    # Create new write file\n",
    "    New_File = open(str(New_file_name) + '.txt','w')\n",
    "    \n",
    "    # Identify text files to retreive\n",
    "    Text_files = (file for file in Dir_list if 'txt.' in file)  # attempt to use a generator. \n",
    "\n",
    "    # Create Loop Through List of Directories\n",
    "    for x in Text_files:\n",
    "        File = open(x, 'rb')\n",
    "        Text = File.read()\n",
    "        \n",
    "        # Write files to new file\n",
    "        New_File.write(str(Text))\n",
    "        New_File.write('\\n')\n",
    "    # Close File\n",
    "    New_File.close()\n",
    "\n",
    "def write_to_text_file(Text_2_write, File_name2_use):\n",
    "    file = open(str(File_name2_use) + '.txt', 'w')    \n",
    "    file.write(Text_2_write)   \n",
    "\n",
    "    \n",
    "def get_cleaned_concatenated_text_file(Dir_list):\n",
    "    '''\n",
    "    Input  = List of files in the directory\n",
    "    Output = Cleaned text \n",
    "    \n",
    "    '''\n",
    "    # Note, the author assumes there is only one Concat file in the dir.  Since the order of the files in the dir\n",
    "    # can change, the better approach is to identify it using a list comprehension with an if statement. \n",
    "    Dirty_text_loc = (file for file in Dir_list if 'Concatenated' in file)\n",
    "       \n",
    "    Concat_file = next(Dirty_text_loc)      \n",
    "    File = open(Concat_file)\n",
    "    # Read in dirty text\n",
    "    Text_dirty = File.read()\n",
    "    # Run cleaning pipeline\n",
    "    Clean_text = text_clearning_pipeline_Input_4_Error_Checker_Function(Text_dirty)\n",
    "    \n",
    "    # Return cleaned text\n",
    "    return Clean_text\n",
    "\n",
    "\n",
    "def create_Wordnet_set():\n",
    "    '''The purpose of this function is to create a set of all words from the wordnet dictionary.\n",
    "    Input  = None\n",
    "    Output = Set object of all words. \n",
    "    '''\n",
    "    # Import words from wordnet\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    Words = wn.words()\n",
    "\n",
    "    # Create List to capture words  \n",
    "    List_dict_words = []; [List_dict_words.append(x) for x in Words]\n",
    "    \n",
    "    # Create Set\n",
    "    Set_dict_words = set(List_dict_words)\n",
    "    \n",
    "    # Return Set\n",
    "    return Set_dict_words\n",
    "\n",
    "def get_set_from_text(Dir_list):\n",
    "    '''The purpose of this code is to create a set of unique tokens from a text file as a string object. \n",
    "    Input  =  Text file as a string object \n",
    "    Output =  Set of unique tokens. \n",
    "    '''\n",
    "    # Define Set Object\n",
    "    Create_set = ''\n",
    "    \n",
    "    # Obtain Your Target File\n",
    "    Target_file = (file for file in Dir_list if 'Cleaned' in file)\n",
    "    \n",
    "    # Loop over Target_file since it is a generator object. \n",
    "    Concat_file = next(Target_file)\n",
    "    File = open(Concat_file)\n",
    "    Text = File.read()\n",
    "    # Tokenize Text\n",
    "    Text_tokenized = nltk.word_tokenize(Text)\n",
    "    # Create Set\n",
    "    Create_set = set(Text_tokenized)\n",
    "    # Return Set\n",
    "    return Create_set\n",
    "\n",
    "\n",
    "def correct_tokens_nABC_using_wordnet_dict(Token_list):\n",
    "    '''The purpose of this code is to '''\n",
    "    \n",
    "    # Creat a clean list of tokens to return to the user. \n",
    "    Token_list_cleaned = []\n",
    "    \n",
    "    # Convert tokens to lowercase\n",
    "    Token_list_lower = [x.lower() for x in Token_list]\n",
    "    \n",
    "    # Loop over the list of tokens\n",
    "    for token in Token_list_lower:\n",
    "        # Find the tokens that start with an 'n'\n",
    "        if token[0] == 'n':\n",
    "            # See if the token is in the WordNet Dict when the 'n' is dropped\n",
    "            if token[1:] in Wordnet_set:\n",
    "                # If the token is in the dictionary, append the token without the 'n'\n",
    "                Token_list_cleaned.append(token[1:])\n",
    "            else:\n",
    "                # If not, then just append the token as there was no matching word. \n",
    "                Token_list_cleaned.append(token)\n",
    "                \n",
    "        # If the token does not start with an 'n', then this code does not apply and append back to the list. \n",
    "        else:\n",
    "            Token_list_cleaned.append(token)\n",
    "    \n",
    "    return Token_list_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TEXT CLEANING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This pipeline will be placed inside a larger function that loops over the Target Directory, identifies the text files,\n",
    "    opens them, etc, and also captures the target file, tokenized text and statistics.  We'll need to create these \n",
    "    variables within the master function. \n",
    "'''\n",
    "\n",
    "def text_clearning_pipeline_Input_4_Error_Checker_Function(Text_file):\n",
    "    '''The purpose of this function is to prepare text for use with the Error Checker Program\n",
    "    Input  =  Single text file\n",
    "    Output =  List of clean tokens representing a single text. \n",
    "    '''\n",
    "    # Run Clearning Pipeline (These functions are taken from the ones define above)\n",
    "    txt_strip_backslashes = clean_text_4_classification_remove_backslashes(Text_file)\n",
    "    txt_strip_nABC = clean_text_4_classification_remove_nABC(txt_strip_backslashes)\n",
    "    txt_strip_punct = strip_punctuation(txt_strip_nABC)\n",
    "    txt_strip_2_letter_words = strip_two_letter_words(txt_strip_punct)\n",
    "    txt_strip_stop_words = strip_stop_words(txt_strip_2_letter_words)\n",
    "    txt_correct_nABC_using_wordnet = correct_tokens_nABC_using_wordnet_dict(txt_strip_stop_words)\n",
    "    \n",
    "    # Rejoin the tokens into a text so that we can write the text to a file.  This way we don't need to run this \n",
    "    # code everytime we want to work with the cleaned text. \n",
    "    Text_rejoined = ' '.join(txt_correct_nABC_using_wordnet)\n",
    "    \n",
    "    # Return List of clean tokenized text\n",
    "    return Text_rejoined\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCATENATE TEXT FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The purpose of this code is to concatenate all of the text files in the target directory'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The purpose of this code is to concatenate all of the text files in the target directory'''\n",
    "\n",
    "create_Concatenated_text_file(Dir_list, 'Concatenated Text File - Cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN DIRTY CONCATENATED FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The prupose of this code is to run the Dirty Concatenated text throught the text cleaning pipeline'''\n",
    "\n",
    "Cleaned_concatenated_txt_file = get_cleaned_concatenated_text_file(Dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CLEANED TEXT TO TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_text_file(Cleaned_concatenated_txt_file, 'Concatenated Text File - Cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A SET OF THE WORDS FOUND IN THE CONCATENATED TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set_concat_text = get_set_from_text(Dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET FREQUENCY DISTRIBUTION OF WORDS IN SET BY TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST TEXT CLEARNING PIPELINE (Feel free to test the code on a single text file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "File = Target_dir[1]\n",
    "File_open = open(File, 'rb')\n",
    "Text_bytes = File_open.read()\n",
    "Text_str = str(Text_bytes)\n",
    "Text_tokens_cleaned = text_clearning_pipeline_Input_4_Error_Checker_Function(Text_str)\n",
    "Text_tokens_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR CHECKER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A SET OF WORDS FROM THE WORDNET DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Set_wordnet_dict():\n",
    "    '''The purpose of this function is to create a set of all words from the wordnet dictionary.\n",
    "    Input  = None\n",
    "    Output = Set object of all words. Note that the author chose to use a set as it is supposed to be faster than \n",
    "             iterating over a list. \n",
    "    '''\n",
    "    # Import words from wordnet\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    Words = wn.words()\n",
    "\n",
    "    # Create List to capture words  \n",
    "    List_dict_words = []; [List_dict_words.append(x) for x in Words]\n",
    "    \n",
    "    # Create Set\n",
    "    Set_dict_words = set(List_dict_words)\n",
    "    \n",
    "    # Return Set\n",
    "    return Set_dict_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE ERROR FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Error_Percentage_Per_Document(Tokenized_text_list_tokens):\n",
    "    '''The purpose of this function is to count the tokens in a given text that do not match a word in the Wordnet Dict\n",
    "    Input  = List of tokens\n",
    "    Ouput  = Float that represents the percentage of error in the documents  \n",
    "    \n",
    "    Note:  The final function will map the error percentage to the document name. \n",
    "    '''\n",
    "    \n",
    "    # Create Set of dictionary words\n",
    "    Word_set = create_Set_wordnet_dict()\n",
    "    \n",
    "    # Create Counter & Calc Num of total tokens \n",
    "    Num_tokens = len(Tokenized_text_list_tokens)\n",
    "    Count_false = 0\n",
    "    \n",
    "    # Loop over list to unpack tokenz\n",
    "    for token in Tokenized_text_list_tokens:\n",
    "        \n",
    "    # Check to see if token in Word_set\n",
    "        if token not in Word_set:\n",
    "            # if token in set\n",
    "            Count_false += 1\n",
    "        \n",
    "    # Error percentage is the Num of Errors / Total tokens * 100. \n",
    "    Error_percentage = round(((Count_false / Num_tokens)*100), 2)\n",
    "    \n",
    "    # Return Error_percentage_value \n",
    "    return Error_percentage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.58"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Function on a single text\n",
    "\n",
    "get_Error_Percentage_Per_Document(Text_tokens_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE ERROR FREQUENCY DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Error_Frequency_Distribution(Tokenized_text_list_tokens, Obj_is_list_of_lists = '', Graph = ''):\n",
    "    '''The purpose of this function is generate a frequency distribution of errors for as many tokenized text files as are\n",
    "    passed to the function.  \n",
    "    Input  = a.) Tokenized_text =  Pass a text that has been tokenized. \n",
    "             b.) Obj_is_list =     If a object passed is a list of text, set List_of_texts to True.  \n",
    "                                   Otherwise, defaults to False\n",
    "             c.) Graph =           You can choose to visualize the results as a graph or not. \n",
    "    \n",
    "    Ouput  = a.) Pandas dataframe whose 'key' is the error and 'values' count that this error appears in teh text. \n",
    "             b.) You can also chose to display the dataframe as graph. \n",
    "    '''\n",
    "        \n",
    "    # Create Set of dictionary words\n",
    "    Word_set = create_Set_wordnet_dict()\n",
    "    \n",
    "    # Create Dictionary to Capture Errors (keys) and Count (values)\n",
    "    Error_Freq_Dist_dict = {}\n",
    "    \n",
    "    # Check if List_of_texts True\n",
    "    if Obj_is_list_of_lists is False:\n",
    "    \n",
    "        # Loop over list to unpack tokens\n",
    "        for token in Tokenized_text_list_tokens:\n",
    "            \n",
    "            # Check to see if token in Word_set\n",
    "            if token not in Word_set:\n",
    "                Error_Freq_Dist_dict[token] = Error_Freq_Dist_dict.get(token, 0) +1\n",
    "    else:\n",
    "        # Loop over list of texts:\n",
    "        for token_list in Tokenized_text_list_tokens:\n",
    "            \n",
    "            # Loop over list to unpack tokens\n",
    "            for token in token_list:\n",
    "            \n",
    "                # Check to see if token in Word_set\n",
    "                if token not in Word_set:\n",
    "                    \n",
    "                    Error_Freq_Dist_dict[token] = Error_Freq_Dist_dict.get(token, 0) +1\n",
    "\n",
    "    # Create Pandas Dataframe\n",
    "    import pandas as pd, matplotlib.pyplot as plt\n",
    "    Index = [1]\n",
    "    df = pd.DataFrame(Error_Freq_Dist_dict, index = Index)\n",
    "    df_tp = pd.DataFrame.transpose(df)\n",
    "    df_sorted = df_tp.sort_values(1, ascending = False)\n",
    "                    \n",
    "    # Return a Bar graph if Graph = True\n",
    "    if Graph == True:\n",
    "        graph = df_sorted.plot(kind = 'bar')\n",
    "        return plt.show()\n",
    "    \n",
    "    # Otherwise, return a dataframe. \n",
    "    else:\n",
    "        # Return Error_percentage_value \n",
    "        return df_sorted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAFOCAYAAAB9tV2bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23a0b661d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Code\n",
    "\n",
    "get_Error_Frequency_Distribution(Text_tokens_cleaned, Obj_is_list_of_lists= False, Graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL PIPELINE CODE FOR THIS PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Error_Checker_PDF_2_TXT(Target_dir, Get_Error_Percentage_Per_Doc = '', Get_Freq_Dist_Errors_All_Docs = '', Graph = ''):\n",
    "    '''\n",
    "    Input  =    Target directory     =    Input the target directory (obj or string) where the text files are saved. \n",
    "                                          Set a slicer if you  want to limit the files from the Target_dir. \n",
    "                Get_Error_Percentage =    Set this option to True if you want to obtain a dataframe of the Error % per \n",
    "                                          text file. \n",
    "                Get_Freq_Dist_Errors =    Set this option to True if you want to obtain a dataframe with the frequency of \n",
    "                                          errors for the text files that are in your target directory. \n",
    "                \n",
    "                Note:  You can only set one of the two functions to True.  \n",
    "                \n",
    "    Output =    This function allows the user to choose which of the two functions to run a. Error % or Freq_Dist.\n",
    "                The user may also chose to display the dataframe returned from the Get_Freq_Dist function as a graph. \n",
    "    '''\n",
    "    \n",
    "    # Create Lists And Objects to Capture Values Generated From the Subjecuent Functions\n",
    "    List_Text_files_cleaned = []\n",
    "    List_Text_file_names = []\n",
    "    Dict_filename_error_percentage = {}\n",
    "    Error_Freq_Distr = ''\n",
    "    \n",
    "    # Get Text Files in Dir Only\n",
    "    get_txt_files_only_in_dir = [file for file in Target_dir if '.txt' in file]\n",
    "    \n",
    "    \n",
    "    # Loop over directory, Identify Text Documents, Open+Read Text Documents\n",
    "    for File in get_txt_files_only_in_dir:\n",
    "        \n",
    "        # Convert Files to Text & Append Text to List\n",
    "        File_open = open(File, 'rb')\n",
    "        Text_bytes = File_open.read()\n",
    "        Text_str = str(Text_bytes)\n",
    "        \n",
    "        \n",
    "        # FUNCTION #1 - TEXT CLEARNING PIPELINE\n",
    "        \n",
    "        get_cleaned_text_tokenized = text_clearning_pipeline_Input_4_Error_Checker_Function(Text_str)\n",
    "        \n",
    "        # Append Cleaned Text to List to be used for the Get Frequency Distribution Function. \n",
    "        List_Text_files_cleaned.append(get_cleaned_text_tokenized)\n",
    "        \n",
    "        \n",
    "        # FUNCTION #2:  GET ERROR PERCENTAGE PER DOC\n",
    "        \n",
    "        # Only run this function only if the input is set to 'True'\n",
    "        if Get_Error_Percentage_Per_Doc == True:\n",
    "            \n",
    "            # Import Error Function\n",
    "            get_file_error_percentage = get_Error_Percentage_Per_Document(get_cleaned_text_tokenized)\n",
    "            get_file_name = str(File)\n",
    "        \n",
    "            # Create Dictionary File Name + Error %\n",
    "            Dict_filename_error_percentage[get_file_name] = get_file_error_percentage\n",
    "    \n",
    "    \n",
    "        # FUNCTION #3: GET FREQUENCY DISTRIBUTION OF ERRORS FOR ALL DOCS\n",
    "        \n",
    "        # Only run this function is the input is set to 'True'\n",
    "        if Get_Freq_Dist_Errors_All_Docs == True:\n",
    "            \n",
    "            # Import Error Freq Distribution Function           \n",
    "            Error_Freq_Distr = get_Error_Frequency_Distribution(List_Text_files_cleaned, \n",
    "                                                                Obj_is_list_of_lists= True, \n",
    "                                                                Graph = Graph)\n",
    "                                                                #Graph = Graph so that this sub function pulls from the\n",
    "                                                                         #input set in the main function\n",
    "    # DETERMINE WHICH OUTPUT TO RETURN TO THE USER\n",
    "    \n",
    "    if Get_Error_Percentage_Per_Doc == True:\n",
    "        \n",
    "        return Dict_filename_error_percentage\n",
    "    \n",
    "    elif Get_Freq_Dist_Errors_All_Docs == True and Graph == False:\n",
    "        \n",
    "            return Error_Freq_Distr\n",
    "    \n",
    "    elif Get_Freq_Dist_Errors_All_Docs == True and Graph == True:\n",
    "    \n",
    "            Error_Freq_Distr.plot(kind = 'bar')\n",
    "            return plt.show()\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x80</th>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xe2</th>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x99s</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filed</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xc2</th>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x9d</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nplaintiff</th>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xef</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xac</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xa7</th>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>defendants</th>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x99</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u.s.c</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nthe</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flsa</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndefendant</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plaintiffs</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blalock</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chung</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1:15-cv</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worked</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>including</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nin</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inc.</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pittman</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employees</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndefendants</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bryant</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/08/15</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/09/15</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n6880921</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n6184969</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n6.case</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n58</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nadvertised</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nadvised</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naffidavits</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nafrican-americans</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasserting</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nassert</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasked</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nare</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nappropriate</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>napprising</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>napplying</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>napply</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nany</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nangela</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nanderson</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nanchor</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>namounts</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>namerican</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>namended-</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>named</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nambitions</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nam</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nallegation</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nalabama</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nage</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zucker</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2158 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      1\n",
       "x80                 695\n",
       "xe2                 695\n",
       "x99s                133\n",
       "filed               133\n",
       "xc2                 129\n",
       "x9d                 128\n",
       "nplaintiff          116\n",
       "xef                 110\n",
       "xac                 110\n",
       "xa7                 107\n",
       "defendants          105\n",
       "x99                  87\n",
       "u.s.c                83\n",
       "nthe                 77\n",
       "flsa                 75\n",
       "ndefendant           72\n",
       "plaintiffs           68\n",
       "blalock              60\n",
       "chung                49\n",
       "1:15-cv              44\n",
       "worked               42\n",
       "including            40\n",
       "nin                  37\n",
       "inc.                 37\n",
       "pittman              35\n",
       "employees            35\n",
       "ndefendants          32\n",
       "bryant               31\n",
       "12/08/15             30\n",
       "12/09/15             30\n",
       "...                 ...\n",
       "n6880921              1\n",
       "n6184969              1\n",
       "n6.case               1\n",
       "n58                   1\n",
       "nadvertised           1\n",
       "nadvised              1\n",
       "naffidavits           1\n",
       "nafrican-americans    1\n",
       "nasserting            1\n",
       "nassert               1\n",
       "nasked                1\n",
       "nare                  1\n",
       "nappropriate          1\n",
       "napprising            1\n",
       "napplying             1\n",
       "napply                1\n",
       "nany                  1\n",
       "nangela               1\n",
       "nanderson             1\n",
       "nanchor               1\n",
       "namounts              1\n",
       "namerican             1\n",
       "namended-             1\n",
       "named                 1\n",
       "nambitions            1\n",
       "nam                   1\n",
       "nallegation           1\n",
       "nalabama              1\n",
       "nage                  1\n",
       "zucker                1\n",
       "\n",
       "[2158 rows x 1 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Code\n",
    "\n",
    "Error_Checker_PDF_2_TXT(Target_dir, \n",
    "                        Get_Error_Percentage_Per_Doc= False, \n",
    "                        Get_Freq_Dist_Errors_All_Docs= True, \n",
    "                        Graph = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
