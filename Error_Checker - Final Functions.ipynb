{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preparation for Error Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define The Target Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\Chris.Cirelli\\Desktop\\Python Programming Docs\\GSU\\Sprint Project\\Legal Docs')\n",
    "Target_dir = os.listdir(r'C:\\Users\\Chris.Cirelli\\Desktop\\Python Programming Docs\\GSU\\Sprint Project\\Legal Docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'GA_Northern_1_15-cv-04247-TWT_26.txt',\n",
       " 'GA_Northern_1_15-cv-04247-TWT_32.txt',\n",
       " 'GA_Northern_1_15-cv-04249-ODE_4.txt',\n",
       " 'GA_Northern_1_15-cv-04258-AT_0.txt',\n",
       " 'GA_Northern_1_15-cv-04260-CC_0.txt',\n",
       " 'GA_Northern_1_15-cv-04260-CC_8.txt',\n",
       " 'GA_Northern_1_15-cv-04264-AT_0.txt',\n",
       " 'GA_Northern_1_15-cv-04281-RWS_0.txt',\n",
       " 'GA_Northern_1_15-cv-04284-WSD_0.txt',\n",
       " 'GA_Northern_1_15-cv-04285-AT_0.txt',\n",
       " 'GA_Northern_1_15-cv-04288-AT_0.txt',\n",
       " 'GA_Northern_1_15-cv-04298-MHC_0.txt',\n",
       " 'GA_Northern_1_15-cv-04303-ODE_0.txt',\n",
       " 'GA_Northern_1_15-cv-04310-LMM_0.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Personal Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_4_classification_remove_backslashes(Text_file):\n",
    "    '''The purpose of this function is to clean the text files of numerous instances of backslashes \n",
    "    in order to prepare them for the regex expression search. \n",
    "    \n",
    "    Input  =   Single text file \n",
    "    Output =   Single text file cleaned \n",
    "    '''\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    Text_file_lower = Text_file.lower()\n",
    "    \n",
    "    # Split any values in the text on the backslash.  The Text_split_slash should return a list. \n",
    "    Text_split_slash = Text_file_lower.split('\\\\')\n",
    "        \n",
    "    # Return the list to a text. \n",
    "    Text_rejoined = ' '.join(Text_split_slash)\n",
    "                \n",
    "    # Return a list of the cleaned text docs. \n",
    "    return Text_rejoined\n",
    "\n",
    "\n",
    "def clean_text_4_classification_remove_nABC(Text_file):\n",
    "    '''The purpose of this function is to remove the 'n' that appears before words that begin with an upper case letter.  \n",
    "    Input  =   Single txt file\n",
    "    Output =   Clean list of tokens from original txt file\n",
    "   \n",
    "    '''\n",
    "    # Define the regex expression that you want to search for. \n",
    "    Regex_exp = re.compile('n[A-Z*]')\n",
    "    \n",
    "    # Create a list to capture the tokens once they are cleaned \n",
    "    Text_tokenized_cleaned = []\n",
    "            \n",
    "    # Tokenize the given text\n",
    "    Text_tokenized = nltk.word_tokenize(Text_file)\n",
    "            \n",
    "    # Run for loop over tokens for a given text. \n",
    "    for token in Text_tokenized:\n",
    "\n",
    "        # Search for the regex expression\n",
    "        Regex_search = re.search(Regex_exp, token)\n",
    "                \n",
    "        # Test if there was match (None = no match)\n",
    "        if Regex_search != None:\n",
    "                     \n",
    "            # If there was a match, take all letters after the 'n'.   \n",
    "            token_cleaned = token[1:]\n",
    "                    \n",
    "            Text_tokenized_cleaned.append(token_cleaned)\n",
    "                        \n",
    "        # If the Regex_search returned None, return the token back to the Text_tokenized_cleaned list\n",
    "        else:\n",
    "            Text_tokenized_cleaned.append(token)\n",
    "    \n",
    "    # Return a list of clean tokens\n",
    "    return Text_tokenized_cleaned\n",
    "\n",
    "\n",
    "def create_dict_punct():\n",
    "    '''The purpose of this function is to simply create a dictionary of punctuation symbols to use\n",
    "    in other functions'''\n",
    "    import string\n",
    "    Dict = {}\n",
    "    Punct = string.punctuation\n",
    "    for x in Punct:\n",
    "        Dict[x] = ''\n",
    "    return Dict \n",
    "\n",
    "def strip_punctuation(Token_list):\n",
    "    '''The purpose of this function is to strip the punctuation from a list of tokens. \n",
    "    Input  =  List of tokens\n",
    "    Output =  List of tokens absent punctuation.  \n",
    "    '''\n",
    "    \n",
    "    # Import punctuation dictionary\n",
    "    Dict_punct = create_dict_punct()\n",
    "\n",
    "    # Create a list to capture the cleaned tokens\n",
    "    Clean_token_list = []    \n",
    "        \n",
    "    # Iterate over the tokens in the txt file\n",
    "    for x in Token_list:\n",
    "        if x not in Dict_punct:\n",
    "            # Append tokens to clean token list\n",
    "            Clean_token_list.append(x)\n",
    "    \n",
    "    # Return a list of cleaned text\n",
    "    return Clean_token_list\n",
    "\n",
    "def strip_two_letter_words(Token_list):\n",
    "    '''The purpose of this function is to remove any two letter tokens from a list of tokens.\n",
    "    Input  =   List of tokens\n",
    "    Output =   List of tokens absent two letter words'''\n",
    "    \n",
    "    List = [x for x in Token_list if len(x) > 2]\n",
    "    \n",
    "    return List\n",
    "\n",
    "def create_dict_stopwords():\n",
    "    from nltk.corpus import stopwords\n",
    "    Stopwords = stopwords.words('english')                  \n",
    "    Dict = {}\n",
    "    for x in Stopwords:\n",
    "        Dict[x] = ''\n",
    "    return Dict\n",
    "\n",
    "def strip_stop_words(Token_list):\n",
    "    stop_words = create_dict_stopwords()\n",
    "    List = []\n",
    "    for x in Token_list:\n",
    "        if x not in stop_words:\n",
    "            List.append(x)\n",
    "    return List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This pipeline will be placed inside a larger function that loops over the Target Directory, identifies the text files,\n",
    "    opens them, etc, and also captures the target file, tokenized text and statistics.  We'll need to create these \n",
    "    variables within the master function. \n",
    "'''\n",
    "\n",
    "def text_clearning_pipeline_Input_4_Error_Checker_Function(Text_file):\n",
    "    '''The purpose of this function is to prepare text for use with the Error Checker Program\n",
    "    Input  =  Single text file\n",
    "    Output =  List of clean tokens representing a single text. \n",
    "    '''\n",
    "    \n",
    "    # Run Clearning Pipeline\n",
    "    txt_strip_backslashes = clean_text_4_classification_remove_backslashes(Text_file)\n",
    "    txt_strip_nABC = clean_text_4_classification_remove_nABC(txt_strip_backslashes)\n",
    "    txt_strip_punct = strip_punctuation(txt_strip_nABC)\n",
    "    txt_strip_2_letter_words = strip_two_letter_words(txt_strip_punct)\n",
    "    txt_strip_stop_words = strip_stop_words(txt_strip_2_letter_words)\n",
    "        \n",
    "    # Return List of clean tokenized text\n",
    "    return txt_strip_stop_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Final Function Setup to test Clearning Pipeliine Pipeline (mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "File = Target_dir[1]\n",
    "File_open = open(File, 'rb')\n",
    "Text_bytes = File_open.read()\n",
    "Text_str = str(Text_bytes)\n",
    "Text_tokens_cleaned = text_clearning_pipeline_Input_4_Error_Checker_Function(Text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'case\",\n",
       " '1:15-cv-04247-twt',\n",
       " 'document',\n",
       " 'filed',\n",
       " '10/24/16',\n",
       " 'page',\n",
       " 'nin',\n",
       " 'united',\n",
       " 'states',\n",
       " 'district']"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Cleaning Pipeline\n",
    "Text_tokens_cleaned[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR CHECKER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Set of Wordnet Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Set_wordnet_dict():\n",
    "    '''The purpose of this function is to create a set of all words from the wordnet dictionary.\n",
    "    Input  = None\n",
    "    Output = Set object of all words. \n",
    "    '''\n",
    "    # Import words from wordnet\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    Words = wn.words()\n",
    "\n",
    "    # Create List to capture words  \n",
    "    List_dict_words = []; [List_dict_words.append(x) for x in Words]\n",
    "    \n",
    "    # Create Set\n",
    "    Set_dict_words = set(List_dict_words)\n",
    "    \n",
    "    # Return Set\n",
    "    return Set_dict_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Error Counting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Error_Percentage_Per_Document(Tokenized_text_list_tokens):\n",
    "    '''The purpose of this function is to count the tokens in a given text that do not match a word in the Wordnet Dict\n",
    "    Input  = List of tokens\n",
    "    Ouput  = Float that represents the percentage of error in the documents  \n",
    "    \n",
    "    Note:  the final function will need to map the error percentage to the document name. \n",
    "    '''\n",
    "    \n",
    "    # Create Set of dictionary words\n",
    "    Word_set = create_Set_wordnet_dict()\n",
    "    \n",
    "    # Create Counter & Calc Num of total tokens \n",
    "    Num_tokens = len(Tokenized_text_list_tokens)\n",
    "    Count_false = 0\n",
    "    \n",
    "    # Loop over list to unpack tokenz\n",
    "    for token in Tokenized_text_list_tokens:\n",
    "        \n",
    "    # Check to see if token in Word_set\n",
    "        if token not in Word_set:\n",
    "            # if token in set\n",
    "            Count_false += 1\n",
    "        \n",
    "    # Error percentage is the Num of Errors / Total tokens * 100. \n",
    "    Error_percentage = round(((Count_false / Num_tokens)*100), 2)\n",
    "    \n",
    "    # Return Error_percentage_value \n",
    "    return Error_percentage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.58"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Error_Percentage_Per_Document(Text_tokens_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Error Frequency Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Error_Frequency_Distribution(Tokenized_text_list_tokens, Obj_is_list_of_lists = '', Graph = ''):\n",
    "    '''The purpose of this function is generate a frequency distribution of errors for as many tokenized text files as are\n",
    "    passed to the function.  \n",
    "    Input  = Tokenized text. If a object passed is a list of text, set List_of_texts to True.  Otherwise, defaults to False\n",
    "    Ouput  = Dictionary whose 'key' is the error and 'values' is the   \n",
    "    '''\n",
    "        \n",
    "    # Create Set of dictionary words\n",
    "    Word_set = create_Set_wordnet_dict()\n",
    "    \n",
    "    # Create Dictionary to Capture Errors (keys) and Count (values)\n",
    "    Error_Freq_Dist_dict = {}\n",
    "    \n",
    "    # Check if List_of_texts True\n",
    "    if Obj_is_list_of_lists is False:\n",
    "    \n",
    "        # Loop over list to unpack tokens\n",
    "        for token in Tokenized_text_list_tokens:\n",
    "            \n",
    "            # Check to see if token in Word_set\n",
    "            if token not in Word_set:\n",
    "                Error_Freq_Dist_dict[token] = Error_Freq_Dist_dict.get(token, 0) +1\n",
    "    else:\n",
    "        # Loop over list of texts:\n",
    "        for token_list in Tokenized_text_list_tokens:\n",
    "            \n",
    "            # Loop over list to unpack tokens\n",
    "            for token in token_list:\n",
    "            \n",
    "                # Check to see if token in Word_set\n",
    "                if token not in Word_set:\n",
    "                    \n",
    "                    Error_Freq_Dist_dict[token] = Error_Freq_Dist_dict.get(token, 0) +1\n",
    "\n",
    "    # Create Pandas Dataframe\n",
    "    import pandas as pd, matplotlib.pyplot as plt\n",
    "    Index = [1]\n",
    "    df = pd.DataFrame(Error_Freq_Dist_dict, index = Index)\n",
    "    df_tp = pd.DataFrame.transpose(df)\n",
    "    df_sorted = df_tp.sort_values(1, ascending = False)\n",
    "                    \n",
    "    if Graph == True:\n",
    "        graph = df_sorted.plot(kind = 'bar')\n",
    "        return plt.show()\n",
    "    \n",
    "    else:\n",
    "        # Return Error_percentage_value \n",
    "        return df_sorted\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test Error Freq Dist for a list of text\n",
    "    \n",
    "List_clean_txt_tokens = []\n",
    "\n",
    "for Dir in Target_dir:\n",
    "    File_open = open(File, 'rb')\n",
    "    Text_bytes = File_open.read()\n",
    "    Text_str = str(Text_bytes)\n",
    "    Text_tokens_cleaned = text_clearning_pipeline_Input_4_Error_Checker_Function(Text_str)\n",
    "    List_clean_txt_tokens.append(Text_tokens_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x80</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xe2</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x99s</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nthe</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>states</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xef</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xac</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016.</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norder</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x94cv-04247-</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x81ling</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x81le</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnorthern</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twt</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nthomas</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nresponse</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nplaintiff</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npittman</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/24/16</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1:15</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>njudge</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nin</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nhimself</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nfor</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndefendants</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n/s/thomas</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mosely</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llc</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grants</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filed</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entered</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consultants</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b'case</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1:15-cv-04247-twt</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nmichael</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    1\n",
       "x80                60\n",
       "xe2                60\n",
       "x99s               45\n",
       "nthe               30\n",
       "states             30\n",
       "xef                30\n",
       "xac                30\n",
       "2016.              30\n",
       "norder             30\n",
       "x94cv-04247-       15\n",
       "x81ling            15\n",
       "x81le              15\n",
       "nnorthern          15\n",
       "twt                15\n",
       "nthomas            15\n",
       "nresponse          15\n",
       "nplaintiff         15\n",
       "npittman           15\n",
       "10/24/16           15\n",
       "1:15               15\n",
       "njudge             15\n",
       "nin                15\n",
       "nhimself           15\n",
       "nfor               15\n",
       "ndefendants        15\n",
       "n/s/thomas         15\n",
       "mosely             15\n",
       "llc                15\n",
       "grants             15\n",
       "filed              15\n",
       "entered            15\n",
       "consultants        15\n",
       "b'case             15\n",
       "1:15-cv-04247-twt  15\n",
       "nmichael           15"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Error_Frequency_Distribution(List_clean_txt_tokens, Obj_is_list_of_lists= True, Graph = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sub functions of the error checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single document:\n",
    "\n",
    "1. Count of errors\n",
    "2. List of enumerated errors\n",
    "3. Len of document\n",
    "4. % of Error of Document\n",
    "5. Document type?\n",
    "6. Write to Excel. \n",
    "\n",
    "Statistics For All Docs (Separate Function)\n",
    "1. Frequency distribution of errors across all documents\n",
    "2. % Error by document type\n",
    "3. Ranking of documents by highest to lowest quality\n",
    "4. Standard deviation of document (will require mean of all docs). \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FINAL FUNCTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Obtain the metrics we are looking for for only one text.  Then we can run the larger pipeliine function over a\n",
    "directory and capture the values in a dataframe'''\n",
    "\n",
    "def Error_checker(Text_File):\n",
    "    '''The purpose of this function is to identify \n",
    "    a.) The % of errors in a text files translated from a pdf using tesseract OCR\n",
    "    b.) The frequency of the errors in any given text or series of text'''\n",
    "    \n",
    "    # Clearning Pipeline\n",
    "    text_clearning_pipeline_Input_4_Error_Checker_Function(None)\n",
    "    \n",
    "    # Error detection pipeline\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
